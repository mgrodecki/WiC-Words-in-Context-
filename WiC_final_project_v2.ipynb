{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaFgi8LeluP2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9r6NIXPkdk8"
      },
      "source": [
        "!pip install pytorch-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKkSu6q1lTco",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1cd6c00-a832-4536-a762-fbd5cf3d9ea0"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from pytorch_transformers import *\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import string\n",
        "import copy\n",
        "import random\n",
        "from google.colab import drive\n",
        "drive.mount('/data', force_remount=True)\n",
        "BATCH_SIZE = 64\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /data\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFrrDdukUvwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0a7c178-5910-4f7f-ed0f-db09a38d6afa"
      },
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "#find position of a word in a tokenized sentence\n",
        "def position(w, ids):\n",
        "  token=[]\n",
        "  #encode word\n",
        "  wtk = tokenizer.encode(w)\n",
        "  if len(wtk) > 1: #if multiple tokens assign first one\n",
        "    token = [wtk[0]]\n",
        "  else:\n",
        "    token = wtk\n",
        "  for i in range(len(ids)):\n",
        "    if ids[i:i+1] == token:\n",
        "      return i\n",
        "  return -1\n",
        "\n",
        "\n",
        "#find positions of both words in a tokenized sentence\n",
        "def word_positions(wL,ids):\n",
        "  posL=[]\n",
        "  i=0\n",
        "  for w in wL:\n",
        "    pos = position(w,ids[i:])\n",
        "    posL.append(pos+i)\n",
        "    i = posL[-1] + 1\n",
        "  return posL\n",
        "\n",
        "\n",
        "#read all data, store individual elements, tokenize each sentence\n",
        "def read_and_tokenize(d):\n",
        "  sentences=[]\n",
        "  tk_sentences=[]\n",
        "  words=[]\n",
        "  idxs=[]\n",
        "  sentences1=[]\n",
        "  sentences2=[]\n",
        "  labels=[]\n",
        "  starts1=[]\n",
        "  starts2=[]\n",
        "  ends1=[]\n",
        "  ends2=[]\n",
        "  for i, line in enumerate(d):\n",
        "    #store each element line-by-line in a list\n",
        "    words.append(line['word'])\n",
        "    idxs.append(i)\n",
        "    sentences1.append(line['sentence1'])\n",
        "    sentences2.append(line['sentence2'])\n",
        "    if line['label']:\n",
        "      labels.append(1)\n",
        "    else:\n",
        "      labels.append(0)\n",
        "    starts1.append(line['start1'])\n",
        "    starts2.append(line['start2'])\n",
        "    ends1.append(line['end1'])\n",
        "    ends2.append(line['end2'])\n",
        "    #combine both sentences into a single one and store in a list\n",
        "    s = \"<s>\"+line['sentence1']+\"</s><s>\"+line['sentence2']+\"</s>\"\n",
        "    sentences.append(s)\n",
        "    #tokenize and store in a list\n",
        "    tk = tokenizer.encode(s)\n",
        "    tk_sentences.append(tk)\n",
        "  #find the max length of tokenized combined sentences\n",
        "  max_len = len(max(tk_sentences, key=len))\n",
        "  return max_len, sentences, tk_sentences, words, idxs, sentences1, sentences2, labels, starts1, starts2, ends1, ends2\n",
        "\n",
        "\n",
        "#build one hot encoded word location vectors\n",
        "def build_word_loc_vectors(sentences1, sentences2, starts1, starts2, ends1, ends2, tk_sentences, max_len):\n",
        "  word1_locs_vector=[]\n",
        "  word2_locs_vector=[]\n",
        "  for s1, s2, st1, st2, e1, e2, tk in zip(sentences1, sentences2, starts1, starts2, ends1, ends2, tk_sentences):\n",
        "    #split sentences into individual words\n",
        "    s1_words = s1.split(' ')\n",
        "    s2_words = s2.split(' ')\n",
        "    # Find indexes of 'word' location by skipping one word at a time until 'start1' and 'start2' location is reached\n",
        "    idx = 0\n",
        "    loc1 = 0\n",
        "    for k in range(0, len(s1_words)): #sentence1\n",
        "      idx = idx+len(s1_words[k])+1\n",
        "      if idx >= st1 and idx < e1:\n",
        "        loc1=k+1\n",
        "    idx = 0\n",
        "    loc2 = 0\n",
        "    for k in range(0, len(s2_words)): #sentence2\n",
        "      idx = idx+len(s2_words[k])+1\n",
        "      if idx >= st2 and idx < e2:\n",
        "        loc2=k+1\n",
        "    # Strip punctuation\n",
        "    tb = str.maketrans('', '', string.punctuation)\n",
        "    w1p = s1_words[loc1]\n",
        "    w2p = s2_words[loc2]\n",
        "    w1 = w1p.translate(tb)\n",
        "    w2 = w2p.translate(tb)\n",
        "    # Find word positions in a tokenized sentence\n",
        "    word_pos = word_positions([w1, w2], tk)\n",
        "    # Construct one hot encoded location vectors\n",
        "    word1_loc = []\n",
        "    word2_loc = []\n",
        "    for j in range(0, max_len):\n",
        "      word1_loc.append(0.0)\n",
        "      word2_loc.append(0.0)\n",
        "    word1_loc[word_pos[0]] = 1.0\n",
        "    word2_loc[word_pos[1]] = 1.0\n",
        "    #store in a list\n",
        "    word1_locs_vector.append([word1_loc])\n",
        "    word2_locs_vector.append([word2_loc])\n",
        "  return word1_locs_vector, word2_locs_vector\n",
        "\n",
        "\n",
        "#pad tokenized sentences to max_length and build their masks\n",
        "def build_tk_sentence_mask(tk_sentences, max_len):\n",
        "  tokenized_sentence_mask=[]\n",
        "  tokenized_sentence=[]\n",
        "  for tk in tk_sentences:\n",
        "    # Pad with 0's to max_len\n",
        "    tkp = tk.copy()\n",
        "    zeros = [0] * (max_len - len(tk))\n",
        "    tkp.extend(zeros)\n",
        "    tokenized_sentence.append(tkp)\n",
        "    #build binary mask\n",
        "    mask = [1] * len(tk)\n",
        "    mask.extend(zeros)\n",
        "    tokenized_sentence_mask.append(mask)\n",
        "  return tokenized_sentence, tokenized_sentence_mask\n",
        "\n",
        "\n",
        "#build sentence location binary mask\n",
        "def build_sentence_loc_mask(tk_sentences, max_len):\n",
        "  sentence_loc_mask=[]\n",
        "  for tk in tk_sentences:\n",
        "    idx=0\n",
        "    mask=[]\n",
        "    #find index for second 0 in tokenized input indicating beginning of sentence2 (skip first 0 at the beginning of sentence1)\n",
        "    for i in range(1, len(tk)):\n",
        "      if tk[i] == 0:\n",
        "        idx = i\n",
        "    #build binary mask\n",
        "    for i in range(0, idx):\n",
        "      mask.append(0)\n",
        "    for i in range(idx+1,len(tk)+1):\n",
        "      mask.append(1)\n",
        "    for i in range(len(tk)+2, max_len+2):\n",
        "      mask.append(0)\n",
        "    sentence_loc_mask.append(mask)\n",
        "  return sentence_loc_mask\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 898823/898823 [00:00<00:00, 4714188.95B/s]\n",
            "100%|██████████| 456318/456318 [00:00<00:00, 2670253.74B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQE3Ap84j33V"
      },
      "source": [
        "#load training data\n",
        "train=[]\n",
        "with open('/data/My Drive/data/train.jsonl', 'r') as f:\n",
        "  for line in f:\n",
        "    train.append(json.loads(line))\n",
        "\n",
        "# Read and tokenize\n",
        "train_max_len, train_sentences, train_tk_sentences, train_words, train_idxs, train_sentences1, train_sentences2, train_labels, train_starts1, train_starts2, train_ends1, train_ends2 = read_and_tokenize(train)\n",
        "\n",
        "# Build one hot encoded word location vectors\n",
        "train_word1_locs_vector, train_word2_locs_vector = build_word_loc_vectors(train_sentences1, train_sentences2, train_starts1, train_starts2, train_ends1, train_ends2, train_tk_sentences, train_max_len)\n",
        "\n",
        "#pad tokenized sentences to max_length and build their masks\n",
        "train_tokenized_sentence, train_tokenized_sentence_mask = build_tk_sentence_mask(train_tk_sentences, train_max_len)\n",
        "\n",
        "#build sentence binary mask\n",
        "train_sentence_loc_mask = build_sentence_loc_mask(train_tk_sentences, train_max_len)\n",
        "\n",
        "# Tensorize and make tensor dataset, sampler and loader\n",
        "train_t1 = torch.tensor(train_tokenized_sentence)\n",
        "train_t2 = torch.tensor(train_sentence_loc_mask)\n",
        "train_t3 = torch.tensor(train_tokenized_sentence_mask)\n",
        "train_t4 = torch.tensor(train_labels)\n",
        "train_t5 = torch.tensor(train_word1_locs_vector)\n",
        "train_t6 = torch.tensor(train_word2_locs_vector)\n",
        "train_t7 = torch.tensor(train_idxs)\n",
        "train_data = TensorDataset(train_t1,train_t2,train_t3,train_t4,train_t5,train_t6,train_t7)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "#load dev data\n",
        "dev=[]\n",
        "with open('/data/My Drive/data/dev.jsonl', 'r') as f:\n",
        "  for line in f:\n",
        "    dev.append(json.loads(line))\n",
        "\n",
        "# Read and tokenize\n",
        "dev_max_len, dev_sentences, dev_tk_sentences, dev_words, dev_idxs, dev_sentences1, dev_sentences2, dev_labels, dev_starts1, dev_starts2, dev_ends1, dev_ends2 = read_and_tokenize(dev)\n",
        "\n",
        "# Build one hot encoded word location vectors\n",
        "dev_word1_locs_vector, dev_word2_locs_vector = build_word_loc_vectors(dev_sentences1, dev_sentences2, dev_starts1, dev_starts2, dev_ends1, dev_ends2, dev_tk_sentences, dev_max_len)\n",
        "\n",
        "#pad tokenized sentences to max_length and build their masks\n",
        "dev_tokenized_sentence, dev_tokenized_sentence_mask = build_tk_sentence_mask(dev_tk_sentences, dev_max_len)\n",
        "\n",
        "#build sentence binary mask\n",
        "dev_sentence_loc_mask = build_sentence_loc_mask(dev_tk_sentences, dev_max_len)\n",
        "\n",
        "# Tensorize and make tensor dataset, sampler and loader\n",
        "dev_t1 = torch.tensor(dev_tokenized_sentence)\n",
        "dev_t2 = torch.tensor(dev_sentence_loc_mask)\n",
        "dev_t3 = torch.tensor(dev_tokenized_sentence_mask)\n",
        "dev_t4 = torch.tensor(dev_labels)\n",
        "dev_t5 = torch.tensor(dev_word1_locs_vector)\n",
        "dev_t6 = torch.tensor(dev_word2_locs_vector)\n",
        "dev_t7 = torch.tensor(dev_idxs)\n",
        "dev_data = TensorDataset(dev_t1,dev_t2,dev_t3,dev_t4,dev_t5,dev_t6,dev_t7)\n",
        "dev_sampler = RandomSampler(dev_data)\n",
        "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "#load test data\n",
        "test=[]\n",
        "with open('/data/My Drive/data/test.jsonl', 'r') as f:\n",
        "  for line in f:\n",
        "    test.append(json.loads(line))\n",
        "\n",
        "# Read and tokenize\n",
        "test_max_len, test_sentences, test_tk_sentences, test_words, test_idxs, test_sentences1, test_sentences2, test_labels, test_starts1, test_starts2, test_ends1, test_ends2 = read_and_tokenize(test)\n",
        "\n",
        "# Build one hot encoded word location vectors\n",
        "test_word1_locs_vector, test_word2_locs_vector = build_word_loc_vectors(test_sentences1, test_sentences2, test_starts1, test_starts2, test_ends1, test_ends2, test_tk_sentences, test_max_len)\n",
        "\n",
        "#pad tokenized sentences to max_length and build their masks\n",
        "test_tokenized_sentence, test_tokenized_sentence_mask = build_tk_sentence_mask(test_tk_sentences, test_max_len)\n",
        "\n",
        "#build sentence binary mask\n",
        "test_sentence_loc_mask = build_sentence_loc_mask(test_tk_sentences, test_max_len)\n",
        "\n",
        "# Tensorize and make tensor dataset, sampler and loader\n",
        "test_t1 = torch.tensor(test_tokenized_sentence)\n",
        "test_t2 = torch.tensor(test_sentence_loc_mask)\n",
        "test_t3 = torch.tensor(test_tokenized_sentence_mask)\n",
        "test_t4 = torch.tensor(test_labels)\n",
        "test_t5 = torch.tensor(test_word1_locs_vector)\n",
        "test_t6 = torch.tensor(test_word2_locs_vector)\n",
        "test_t7 = torch.tensor(test_idxs)\n",
        "test_data = TensorDataset(test_t1,test_t2,test_t3,test_t4,test_t5,test_t6,test_t7)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Vw1ZLuAz8LE"
      },
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "#Train, validate and test Roberta\n",
        "def train_val_test_roberta(model, epochs):\n",
        "\n",
        "  #AdamW optimizer\n",
        "  optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
        "\n",
        "  #Train and validate\n",
        "  epoch_number = 0\n",
        "  while epoch_number < epochs:\n",
        "    print(\"Epoch Number: \", epoch_number)\n",
        "    epoch_number = epoch_number + 1\n",
        "\n",
        "    # Training with train set\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_steps = 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Load the data\n",
        "      tokenized_sentence, sentence_loc_mask, tokenized_sentence_mask, labels, word1_locs_vector, word2_locs_vector, index = batch\n",
        "      # Zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "      # Forward pass\n",
        "      loss, logits = model(tokenized_sentence, token_type_ids=None, attention_mask=tokenized_sentence_mask, labels=labels)\n",
        "      # Backward pass\n",
        "      loss.backward()\n",
        "      # Optimizer step\n",
        "      optimizer.step()\n",
        "      tr_loss += loss.item()\n",
        "      nb_tr_steps += 1\n",
        "    print(\"Train loss: \", tr_loss/nb_tr_steps)\n",
        "\n",
        "    # Validation with dev set\n",
        "    model.eval()\n",
        "    eval_accuracy = 0\n",
        "    nb_eval_steps = 0\n",
        "    nb_eval_examples = 0\n",
        "    for batch in dev_dataloader:\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Load the data\n",
        "      #b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index\n",
        "      tokenized_sentence, sentence_loc_mask, tokenized_sentence_mask, labels, word1_locs_vector, word2_locs_vector, index = batch\n",
        "      # Do not to compute or store gradients\n",
        "      with torch.no_grad():\n",
        "        # Forward pass\n",
        "        (loss, logits) = model(tokenized_sentence, token_type_ids=None, attention_mask=tokenized_sentence_mask, labels=labels)\n",
        "      # Move to CPU\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      label_ids = labels.to('cpu').numpy()\n",
        "      # Compare output to labels\n",
        "      tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "      eval_accuracy += tmp_eval_accuracy\n",
        "      nb_eval_steps += 1\n",
        "    print(\"Validation Accuracy: \", eval_accuracy/nb_eval_steps)\n",
        "\n",
        "\n",
        "  # Test test with test set once training and validation loop is completed\n",
        "  eval_loss = 0\n",
        "  eval_accuracy = 0\n",
        "  nb_eval_steps = 0\n",
        "  model.eval()\n",
        "  for batch in test_dataloader:\n",
        "    batch = tuple(t.cuda() for t in batch)\n",
        "    # Load the data\n",
        "    tokenized_sentence, sentence_loc_mask, tokenized_sentence_mask, labels, word1_locs_vector, word2_locs_vector, index = batch\n",
        "    # Do not to compute or store gradients\n",
        "    with torch.no_grad():\n",
        "      # Forward pass\n",
        "      (loss, logits) = model(tokenized_sentence, token_type_ids=None, attention_mask=tokenized_sentence_mask, labels=labels)\n",
        "    # Move to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = labels.cpu().numpy()\n",
        "    # Compare output to labels\n",
        "    b_accuracy = flat_accuracy(logits, label_ids)\n",
        "    eval_loss += loss.item()\n",
        "    eval_accuracy += b_accuracy\n",
        "    nb_eval_steps += 1\n",
        "  print(\"Test: \\tLoss = \", eval_loss/nb_eval_steps, \"\\tAccuracy = \", eval_accuracy/nb_eval_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOok0JJIb_-W",
        "outputId": "ee12b1cb-f76c-4063-b0ec-9bf8230e7df5"
      },
      "source": [
        "#train the top layer of RobertaForSequenceClassification\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
        "model = model.cuda()\n",
        "train_val_test_roberta(model, 2)\n",
        "#train_val_test_roberta(model, 3)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch Number:  0\n",
            "Train loss:  0.6768080426167838\n",
            "Validation Accuracy:  0.6651785714285714\n",
            "Epoch Number:  1\n",
            "Train loss:  0.5578582283062271\n",
            "Validation Accuracy:  0.7489853896103896\n",
            "Test: \tLoss =  0.6018227159976959 \tAccuracy =  0.7034274193548387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DPMVDvERm3f"
      },
      "source": [
        "#add a tunable fully connected tunable top on top of roberta\n",
        "class TunableTop1HL(torch.nn.Module):\n",
        "    def __init__(self, basemodel, D_in=768, H=250, D_out=2):\n",
        "        #Fully-connected network with one hidden layer\n",
        "        super(TunableTop1HL, self).__init__()\n",
        "        self.basemodel = basemodel\n",
        "        self.D_in = D_in\n",
        "        self.H = H\n",
        "        self.D_out = D_out\n",
        "        self.linear1 = torch.nn.Linear(D_in, H, bias = True)\n",
        "        self.linear2 = torch.nn.Linear(H, D_out, bias = True)\n",
        "        #experiment with different loss and activation functions\n",
        "        self.loss_CrossEntropy = torch.nn.CrossEntropyLoss()\n",
        "        self.loss_MSE = torch.nn.MSELoss()\n",
        "        self.activation_ReLU = torch.nn.ReLU()\n",
        "        self.activation_GELU = torch.nn.GELU()\n",
        "        self.activation_tan = torch.nn.Tanh()\n",
        "        self.activation_sigmoid = torch.nn.Sigmoid()\n",
        "        self.activation_softmax = torch.nn.Softmax()\n",
        "\n",
        "    def forward(self, tokenized_sentence, tokenized_sentence_mask, labels, word1_locs_vector, word2_locs_vector):\n",
        "        output, _ = self.basemodel.roberta(input_ids=tokenized_sentence, attention_mask=tokenized_sentence_mask)\n",
        "        size = word1_locs_vector.shape[0]\n",
        "        w1 = torch.matmul(word1_locs_vector, output).view(size, self.D_in)\n",
        "        w2 = torch.matmul(word2_locs_vector, output).view(size, self.D_in)\n",
        "        logits1 = self.activation_ReLU(self.linear1(w1-w2))\n",
        "        logits2 = self.activation_softmax(self.linear2(logits1))\n",
        "        loss = self.loss_CrossEntropy(logits2.view(-1, 2), labels.view(-1))\n",
        "        return (loss, logits2)\n",
        "\n",
        "class TunableTop2HL(torch.nn.Module):\n",
        "    def __init__(self, basemodel, D_in=768, H1=30, H2=10, D_out=2):\n",
        "        #Fully-connected network with two hidden layers\n",
        "        super(TunableTop2HL, self).__init__()\n",
        "        self.basemodel = basemodel\n",
        "        self.D_in = D_in\n",
        "        self.H1 = H1\n",
        "        self.H2 = H2\n",
        "        self.D_out = D_out\n",
        "        self.linear1 = torch.nn.Linear(D_in, H1, bias = True)\n",
        "        self.linear2 = torch.nn.Linear(H1, H2, bias = True)\n",
        "        self.linear3 = torch.nn.Linear(H2, D_out, bias = True)\n",
        "        #experiment with different loss and activation functions\n",
        "        self.loss_CrossEntropy = torch.nn.CrossEntropyLoss()\n",
        "        self.loss_MSE = torch.nn.MSELoss()\n",
        "        self.activation_ReLU = torch.nn.ReLU()\n",
        "        self.activation_GELU = torch.nn.GELU()\n",
        "        self.activation_tan = torch.nn.Tanh()\n",
        "        self.activation_sigmoid = torch.nn.Sigmoid()\n",
        "        self.activation_softmax = torch.nn.Softmax()\n",
        "\n",
        "    def forward(self, tokenized_sentence, tokenized_sentence_mask, labels, word1_locs_vector, word2_locs_vector):\n",
        "        output, _ = self.basemodel.roberta(input_ids=tokenized_sentence, attention_mask=tokenized_sentence_mask)\n",
        "        size = word1_locs_vector.shape[0]\n",
        "        w1 = torch.matmul(word1_locs_vector, output).view(size, self.D_in)\n",
        "        w2 = torch.matmul(word2_locs_vector, output).view(size, self.D_in)\n",
        "        logits1 = self.activation_ReLU(self.linear1(w1-w2))\n",
        "        logits2 = self.activation_ReLU(self.linear2(logits1))\n",
        "        logits3 = self.activation_softmax(self.linear3(logits2))\n",
        "        loss = self.loss_CrossEntropy(logits3.view(-1, 2), labels.view(-1))\n",
        "        return (loss, logits3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOOiTLPRbMeM"
      },
      "source": [
        "#Train, validate and test custome model on top of Roberta\n",
        "def train_val_test_custom(model, epochs):\n",
        "\n",
        "  #AdamW optimizer\n",
        "  optimizer = AdamW(model.parameters(), lr = 1e-5, eps = 1e-8)\n",
        "\n",
        "  #Train and validate\n",
        "  epoch_number = 0\n",
        "  while epoch_number < epochs:\n",
        "    print(\"Epoch Number: \", epoch_number)\n",
        "    epoch_number = epoch_number + 1\n",
        "\n",
        "    # Training with train set\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_steps = 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Load the data\n",
        "      tokenized_sentence, sentence_loc_mask, tokenized_sentence_mask, labels, word1_locs_vector, word2_locs_vector, index = batch\n",
        "      # Zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "      # Forward pass\n",
        "      loss, logits = model(tokenized_sentence=tokenized_sentence, tokenized_sentence_mask=tokenized_sentence_mask, labels=labels, word1_locs_vector=word1_locs_vector, word2_locs_vector=word2_locs_vector)\n",
        "      # Backward pass\n",
        "      loss.backward()\n",
        "      # Optimizer step\n",
        "      optimizer.step()\n",
        "      tr_loss += loss.item()\n",
        "      nb_tr_steps += 1\n",
        "    print(\"Train loss: \", tr_loss/nb_tr_steps)\n",
        "\n",
        "    # Validation with dev set\n",
        "    model.eval()\n",
        "    eval_accuracy = 0\n",
        "    nb_eval_steps = 0\n",
        "    nb_eval_examples = 0\n",
        "    for batch in dev_dataloader:\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      # Load the data\n",
        "      #b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index\n",
        "      tokenized_sentence, sentence_loc_mask, tokenized_sentence_mask, labels, word1_locs_vector, word2_locs_vector, index = batch\n",
        "      # Do not to compute or store gradients\n",
        "      with torch.no_grad():\n",
        "        # Forward pass\n",
        "        loss, logits = model(tokenized_sentence=tokenized_sentence, tokenized_sentence_mask=tokenized_sentence_mask, labels=labels, word1_locs_vector=word1_locs_vector, word2_locs_vector=word2_locs_vector)\n",
        "      # Move to CPU\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      label_ids = labels.to('cpu').numpy()\n",
        "      # Compare output to labels\n",
        "      tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "      eval_accuracy += tmp_eval_accuracy\n",
        "      nb_eval_steps += 1\n",
        "    print(\"Validation Accuracy: \", eval_accuracy/nb_eval_steps)\n",
        "\n",
        "\n",
        "  # Test test with test set once training and validation loop is completed\n",
        "  eval_loss = 0\n",
        "  eval_accuracy = 0\n",
        "  nb_eval_steps = 0\n",
        "  model.eval()\n",
        "  for batch in test_dataloader:\n",
        "    batch = tuple(t.cuda() for t in batch)\n",
        "    # Load the data\n",
        "    tokenized_sentence, sentence_loc_mask, tokenized_sentence_mask, labels, word1_locs_vector, word2_locs_vector, index = batch\n",
        "    # Do not to compute or store gradients\n",
        "    with torch.no_grad():\n",
        "      # Forward pass\n",
        "      loss, logits = model(tokenized_sentence=tokenized_sentence, tokenized_sentence_mask=tokenized_sentence_mask, labels=labels, word1_locs_vector=word1_locs_vector, word2_locs_vector=word2_locs_vector)\n",
        "    # Move to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = labels.cpu().numpy()\n",
        "    # Compare output to labels\n",
        "    b_accuracy = flat_accuracy(logits, label_ids)\n",
        "    eval_loss += loss.item()\n",
        "    eval_accuracy += b_accuracy\n",
        "    nb_eval_steps += 1\n",
        "  print(\"Test: \\tLoss = \", eval_loss/nb_eval_steps, \"\\tAccuracy = \", eval_accuracy/nb_eval_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt4pVzu4gmHQ"
      },
      "source": [
        "#RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks.\n",
        "basemodel = RobertaForSequenceClassification.from_pretrained('roberta-base')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqBke-Vmro1t",
        "outputId": "a9cf8249-eff2-4714-ad2f-d6161c98e1b4"
      },
      "source": [
        "print(\"One hidden layer classification head:\")\n",
        "model1 = TunableTop1HL(basemodel, D_in=768, H=250, D_out=2)\n",
        "model1.cuda()\n",
        "train_val_test_custom(model1, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One hidden layer classification head:\n",
            "Epoch Number:  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss:  0.4036568241028846\n",
            "Validation Accuracy:  0.7713068181818182\n",
            "Epoch Number:  1\n",
            "Train loss:  0.37427076735074005\n",
            "Validation Accuracy:  0.7702922077922079\n",
            "Epoch Number:  2\n",
            "Train loss:  0.3753939983965475\n",
            "Validation Accuracy:  0.755275974025974\n",
            "Epoch Number:  3\n",
            "Train loss:  0.3612583196615871\n",
            "Validation Accuracy:  0.7587256493506492\n",
            "Epoch Number:  4\n",
            "Train loss:  0.3559717279446276\n",
            "Validation Accuracy:  0.78125\n",
            "Epoch Number:  5\n",
            "Train loss:  0.3496341999573044\n",
            "Validation Accuracy:  0.7676542207792207\n",
            "Epoch Number:  6\n",
            "Train loss:  0.3460993970496745\n",
            "Validation Accuracy:  0.7918019480519481\n",
            "Epoch Number:  7\n",
            "Train loss:  0.34648707094071785\n",
            "Validation Accuracy:  0.7706980519480519\n",
            "Epoch Number:  8\n",
            "Train loss:  0.3475825918626182\n",
            "Validation Accuracy:  0.7698863636363635\n",
            "Epoch Number:  9\n",
            "Train loss:  0.3451321079761167\n",
            "Validation Accuracy:  0.7725243506493508\n",
            "Test: \tLoss =  0.6037602365016937 \tAccuracy =  0.7035786290322581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWPMFzu9rkx6",
        "outputId": "a702df77-a460-4c88-9b30-4e2aa3ebd880"
      },
      "source": [
        "print(\"\\nTwo hidden layer classification head:\")\n",
        "model2 = TunableTop2HL(basemodel, D_in=768, H1=30, H2=8, D_out=2)\n",
        "model2.cuda()\n",
        "train_val_test_custom(model2, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Two hidden layer classification head:\n",
            "Epoch Number:  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss:  0.4350385367870331\n",
            "Validation Accuracy:  0.7834821428571429\n",
            "Epoch Number:  1\n",
            "Train loss:  0.3672994167744359\n",
            "Validation Accuracy:  0.7637987012987013\n",
            "Epoch Number:  2\n",
            "Train loss:  0.3591215308708481\n",
            "Validation Accuracy:  0.7790178571428571\n",
            "Epoch Number:  3\n",
            "Train loss:  0.35502799628656123\n",
            "Validation Accuracy:  0.7650162337662338\n",
            "Epoch Number:  4\n",
            "Train loss:  0.35549914346465583\n",
            "Validation Accuracy:  0.7767857142857143\n",
            "Epoch Number:  5\n",
            "Train loss:  0.3554455106016956\n",
            "Validation Accuracy:  0.7688717532467532\n",
            "Epoch Number:  6\n",
            "Train loss:  0.3525361358364926\n",
            "Validation Accuracy:  0.7778003246753247\n",
            "Epoch Number:  7\n",
            "Train loss:  0.34261779241924045\n",
            "Validation Accuracy:  0.7857142857142857\n",
            "Epoch Number:  8\n",
            "Train loss:  0.33851926311661923\n",
            "Validation Accuracy:  0.7830762987012987\n",
            "Epoch Number:  9\n",
            "Train loss:  0.3363152621667596\n",
            "Validation Accuracy:  0.7790178571428571\n",
            "Test: \tLoss =  0.6076936542987823 \tAccuracy =  0.697429435483871\n"
          ]
        }
      ]
    }
  ]
}