{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Morphological_inflections_with_encoder_decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XibEOUd6l66Q",
        "yLGYmjfXl66Q"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0pwt53xl659"
      },
      "source": [
        "# The original Jupyter notebook by Bastings (https://bastings.github.io/annotated_encoder_decoder/) entitled Annotated Encoder-Decoder with Attention  was modified for the SIGMORPHON 2016 task1 (Morphological Reinflections) by Michael Grodecki.\n",
        "\n",
        "The algorithm is based on the the attention-based neural machine translation paper [\"Neural Machine Translation by Jointly Learning to Align and Translate\"](https://arxiv.org/abs/1409.0473) by Bahdanau et al. (2015) and the paper by Kann and Schutze (2016) describing the SIGMORPHON 2016 winning system [\"MED: The LMU System for the SIGMORPHON 2016 Shared Task on Morphological Reinflection\"] (https://aclanthology.org/W16-2010.pdf).\n",
        "\n",
        "\n",
        "# Model Architecture\n",
        "\n",
        "We will model the probability $p(Y\\mid X)$ of a target sequence $Y=(y_1, \\dots, y_{N})$ given a source sequence $X=(x_1, \\dots, x_M)$ directly with a neural network: an Encoder-Decoder.\n",
        "\n",
        "#### Encoder \n",
        "\n",
        "The encoder reads in the source sentence (*at the bottom of the figure*) and produces a sequence of hidden states $\\mathbf{h}_1, \\dots, \\mathbf{h}_M$, one for each source word. These states should capture the meaning of a word in its context of the given sentence.\n",
        "\n",
        "We will use a bi-directional recurrent neural network (Bi-RNN) as the encoder; a Bi-GRU in particular.\n",
        "\n",
        "First of all we **embed** the source words. \n",
        "We simply look up the **word embedding** for each word in a (randomly initialized) lookup table.\n",
        "We will denote the word embedding for word $i$ in a given sentence with $\\mathbf{x}_i$.\n",
        "By embedding words, our model may exploit the fact that certain words (e.g. *cat* and *dog*) are semantically similar, and can be processed in a similar way.\n",
        "\n",
        "Now, how do we get hidden states $\\mathbf{h}_1, \\dots, \\mathbf{h}_M$? A forward GRU reads the source sentence left-to-right, while a backward GRU reads it right-to-left.\n",
        "Each of them follows a simple recursive formula: \n",
        "$$\\mathbf{h}_j = \\text{GRU}( \\mathbf{x}_j , \\mathbf{h}_{j - 1} )$$\n",
        "i.e. we obtain the next state from the previous state and the current input word embedding.\n",
        "\n",
        "The hidden state of the forward GRU at time step $j$ will know what words **precede** the word at that time step, but it doesn't know what words will follow. In contrast, the backward GRU will only know what words **follow** the word at time step $j$. By **concatenating** those two hidden states (*shown in blue in the figure*), we get $\\mathbf{h}_j$, which captures word $j$ in its full sentence context.\n",
        "\n",
        "\n",
        "#### Decoder \n",
        "\n",
        "The decoder (*at the top of the figure*) is a GRU with hidden state $\\mathbf{s_i}$. It follows a similar formula to the encoder, but takes one extra input $\\mathbf{c}_{i}$ (*shown in yellow*).\n",
        "\n",
        "$$\\mathbf{s}_{i} = f( \\mathbf{s}_{i - 1}, \\mathbf{y}_{i - 1}, \\mathbf{c}_i )$$\n",
        "\n",
        "Here, $\\mathbf{y}_{i - 1}$ is the previously generated target word (*not shown*).\n",
        "\n",
        "At each time step, an **attention mechanism** dynamically selects that part of the source sentence that is most relevant for predicting the current target word. It does so by comparing the last decoder state with each source hidden state. The result is a context vector $\\mathbf{c_i}$ (*shown in yellow*).\n",
        "Later the attention mechanism is explained in more detail.\n",
        "\n",
        "After computing the decoder state $\\mathbf{s}_i$, a non-linear function $g$ (which applies a [softmax](https://en.wikipedia.org/wiki/Softmax_function)) gives us the probability of the target word $y_i$ for this time step:\n",
        "\n",
        "$$ p(y_i \\mid y_{<i}, x_1^M) = g(\\mathbf{s}_i, \\mathbf{c}_i, \\mathbf{y}_{i - 1})$$\n",
        "\n",
        "Because $g$ applies a softmax, it provides a vector the size of the output vocabulary that sums to 1.0: it is a distribution over all target words. During test time, we would select the word with the highest probability for our translation.\n",
        "\n",
        "Now, for optimization, a [cross-entropy loss](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy) is used to maximize the probability of selecting the correct word at this time step. All parameters (including word embeddings) are then updated to maximize this probability.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hOp-bQrl66F"
      },
      "source": [
        "!pip install torch numpy matplotlib sacrebleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6UYMqwjl66G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06114f34-5579-40a6-8571-530818afd13f"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "# we will use CUDA if it is available\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE=torch.device('cuda:0') # or set to 'cpu'\n",
        "print(\"CUDA:\", USE_CUDA)\n",
        "print(DEVICE)\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA: True\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VDoIyyiFjgn"
      },
      "source": [
        "## EncoderDecoder class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jux2gwG7l66I"
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.trg_embed = trg_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, src, trg, src_mask, trg_mask, src_lengths, trg_lengths):\n",
        "        \"\"\"Take in and process masked src and target sequences.\"\"\"\n",
        "        encoder_hidden, encoder_final = self.encode(src, src_mask, src_lengths)\n",
        "        return self.decode(encoder_hidden, encoder_final, src_mask, trg, trg_mask)\n",
        "    \n",
        "    def encode(self, src, src_mask, src_lengths):\n",
        "        return self.encoder(self.src_embed(src), src_mask, src_lengths)\n",
        "    \n",
        "    def decode(self, encoder_hidden, encoder_final, src_mask, trg, trg_mask,\n",
        "               decoder_hidden=None):\n",
        "        return self.decoder(self.trg_embed(trg), encoder_hidden, encoder_final,\n",
        "                            src_mask, trg_mask, hidden=decoder_hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6_gnfK1l66I"
      },
      "source": [
        "To keep things easy we also keep the `Generator` class the same. \n",
        "It simply projects the pre-output layer ($x$ in the `forward` function below) to obtain the output layer, so that the final dimension is the target vocabulary size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "defcS_Kal66J"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
        "    def __init__(self, hidden_size, vocab_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-PXu0xcl66K"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "Our encoder is a bi-directional GRU. \n",
        "\n",
        "Because we want to process multiple sentences at the same time for speed reasons (it is more effcient on GPU), we need to support **mini-batches**. Sentences in a mini-batch may have different lengths, which means that the RNN needs to unroll further for certain sentences while it might already have finished for others:\n",
        "\n",
        "```\n",
        "Example: mini-batch with 3 source sentences of different lengths (7, 5, and 3).\n",
        "End-of-sequence is marked with a \"3\" here, and padding positions with \"1\".\n",
        "\n",
        "+---------------+\n",
        "| 4 5 9 8 7 8 3 |\n",
        "+---------------+\n",
        "| 5 4 8 7 3 1 1 |\n",
        "+---------------+\n",
        "| 5 8 3 1 1 1 1 |\n",
        "+---------------+\n",
        "```\n",
        "You can see that, when computing hidden states for this mini-batch, for sentence #2 and #3 we will need to stop updating the hidden state after we have encountered \"3\". We don't want to incorporate the padding values (1s).\n",
        "\n",
        "Luckily, PyTorch has convenient helper functions called `pack_padded_sequence` and `pad_packed_sequence`.\n",
        "These functions take care of masking and padding, so that the resulting word representations are simply zeros after a sentence stops.\n",
        "\n",
        "The code below reads in a source sentence (a sequence of word embeddings) and produces the hidden states.\n",
        "It also returns a final vector, a summary of the complete sentence, by concatenating the first and the last hidden states (they have both seen the whole sentence, each in a different direction). We will use the final vector to initialize the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT1dcR29l66K"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encodes a sequence of word embeddings\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, \n",
        "                          batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        \n",
        "    def forward(self, x, mask, lengths):\n",
        "        \"\"\"\n",
        "        Applies a bidirectional GRU to sequence of embeddings x.\n",
        "        The input mini-batch x needs to be sorted by length.\n",
        "        x should have dimensions [batch, time, dim].\n",
        "        \"\"\"\n",
        "        packed = pack_padded_sequence(x, lengths, batch_first=True)\n",
        "        output, final = self.rnn(packed)\n",
        "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
        "\n",
        "        # we need to manually concatenate the final states for both directions\n",
        "        fwd_final = final[0:final.size(0):2]\n",
        "        bwd_final = final[1:final.size(0):2]\n",
        "        final = torch.cat([fwd_final, bwd_final], dim=2)  # [num_layers, batch, 2*dim]\n",
        "\n",
        "        return output, final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD8pXcPal66L"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "The decoder is a conditional GRU. Rather than starting with an empty state like the encoder, its initial hidden state results from a projection of the encoder final vector. \n",
        "\n",
        "#### Training\n",
        "In `forward` you can find a for-loop that computes the decoder hidden states one time step at a time. \n",
        "Note that, during training, we know exactly what the target words should be! (They are in `trg_embed`.) This means that we are not even checking here what the prediction is! We simply feed the correct previous target word embedding to the GRU at each time step. This is called teacher forcing.\n",
        "\n",
        "The `forward` function returns all decoder hidden states and pre-output vectors. Elsewhere these are used to compute the loss, after which the parameters are updated.\n",
        "\n",
        "#### Prediction\n",
        "For prediction time, for forward function is only used for a single time step. After predicting a word from the returned pre-output vector, we can call it again, supplying it the word embedding of the previously predicted word and the last state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpYYQqCzl66L"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"A conditional RNN decoder with attention.\"\"\"\n",
        "    \n",
        "    def __init__(self, emb_size, hidden_size, attention, num_layers=1, dropout=0.5,\n",
        "                 bridge=True):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.attention = attention\n",
        "        self.dropout = dropout\n",
        "                 \n",
        "        self.rnn = nn.GRU(emb_size + 2*hidden_size, hidden_size, num_layers,\n",
        "                          batch_first=True, dropout=dropout)\n",
        "                 \n",
        "        # to initialize from the final encoder state\n",
        "        self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True) if bridge else None\n",
        "\n",
        "        self.dropout_layer = nn.Dropout(p=dropout)\n",
        "        self.pre_output_layer = nn.Linear(hidden_size + 2*hidden_size + emb_size,\n",
        "                                          hidden_size, bias=False)\n",
        "        \n",
        "    def forward_step(self, prev_embed, encoder_hidden, src_mask, proj_key, hidden):\n",
        "        \"\"\"Perform a single decoder step (1 word)\"\"\"\n",
        "\n",
        "        # compute context vector using attention mechanism\n",
        "        query = hidden[-1].unsqueeze(1)  # [#layers, B, D] -> [B, 1, D]\n",
        "        context, attn_probs = self.attention(\n",
        "            query=query, proj_key=proj_key,\n",
        "            value=encoder_hidden, mask=src_mask)\n",
        "\n",
        "        # update rnn hidden state\n",
        "        rnn_input = torch.cat([prev_embed, context], dim=2)\n",
        "        output, hidden = self.rnn(rnn_input, hidden)\n",
        "        \n",
        "        pre_output = torch.cat([prev_embed, output, context], dim=2)\n",
        "        pre_output = self.dropout_layer(pre_output)\n",
        "        pre_output = self.pre_output_layer(pre_output)\n",
        "\n",
        "        return output, hidden, pre_output\n",
        "    \n",
        "    def forward(self, trg_embed, encoder_hidden, encoder_final, \n",
        "                src_mask, trg_mask, hidden=None, max_len=None):\n",
        "        \"\"\"Unroll the decoder one step at a time.\"\"\"\n",
        "                                         \n",
        "        # the maximum number of steps to unroll the RNN\n",
        "        if max_len is None:\n",
        "            max_len = trg_mask.size(-1)\n",
        "\n",
        "        # initialize decoder hidden state\n",
        "        if hidden is None:\n",
        "            hidden = self.init_hidden(encoder_final)\n",
        "        \n",
        "        # pre-compute projected encoder hidden states\n",
        "        # (the \"keys\" for the attention mechanism)\n",
        "        # this is only done for efficiency\n",
        "        proj_key = self.attention.key_layer(encoder_hidden)\n",
        "        \n",
        "        # here we store all intermediate hidden states and pre-output vectors\n",
        "        decoder_states = []\n",
        "        pre_output_vectors = []\n",
        "        \n",
        "        # unroll the decoder RNN for max_len steps\n",
        "        for i in range(max_len):\n",
        "            prev_embed = trg_embed[:, i].unsqueeze(1)\n",
        "            output, hidden, pre_output = self.forward_step(\n",
        "              prev_embed, encoder_hidden, src_mask, proj_key, hidden)\n",
        "            decoder_states.append(output)\n",
        "            pre_output_vectors.append(pre_output)\n",
        "\n",
        "        decoder_states = torch.cat(decoder_states, dim=1)\n",
        "        pre_output_vectors = torch.cat(pre_output_vectors, dim=1)\n",
        "        return decoder_states, hidden, pre_output_vectors  # [B, N, D]\n",
        "\n",
        "    def init_hidden(self, encoder_final):\n",
        "        \"\"\"Returns the initial decoder state,\n",
        "        conditioned on the final encoder state.\"\"\"\n",
        "\n",
        "        if encoder_final is None:\n",
        "            return None  # start with zeros\n",
        "\n",
        "        return torch.tanh(self.bridge(encoder_final))            \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7eqaboIMXgb"
      },
      "source": [
        "### Attention                                                                                                                                                                               \n",
        "\n",
        "At every time step, the decoder has access to *all* source word representations $\\mathbf{h}_1, \\dots, \\mathbf{h}_M$. \n",
        "An attention mechanism allows the model to focus on the currently most relevant part of the source sentence.\n",
        "The state of the decoder is represented by GRU hidden state $\\mathbf{s}_i$.\n",
        "So if we want to know which source word representation(s) $\\mathbf{h}_j$ are most relevant, we will need to define a function that takes those two things as input.\n",
        "\n",
        "Here we use the MLP-based, additive attention that was used in Bahdanau et al. (2015).\n",
        "\n",
        "We apply an MLP with tanh-activation to both the current decoder state $\\bf s_i$ (the *query*) and each encoder state $\\bf h_j$ (the *key*), and then project this to a single value (i.e. a scalar) to get the *attention energy* $e_{ij}$. \n",
        "\n",
        "Once all energies are computed, they are normalized by a softmax so that they sum to one: \n",
        "\n",
        "$$ \\alpha_{ij} = \\text{softmax}(\\mathbf{e}_i)[j] $$\n",
        "\n",
        "$$\\sum_j \\alpha_{ij} = 1.0$$ \n",
        "\n",
        "The context vector for time step $i$ is then a weighted sum of the encoder hidden states (the *values*):\n",
        "$$\\mathbf{c}_i = \\sum_j \\alpha_{ij} \\mathbf{h}_j$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlXEosrAl66N"
      },
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n",
        "    \n",
        "    def __init__(self, hidden_size, key_size=None, query_size=None):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        \n",
        "        # We assume a bi-directional encoder so key_size is 2*hidden_size\n",
        "        key_size = 2 * hidden_size if key_size is None else key_size\n",
        "        query_size = hidden_size if query_size is None else query_size\n",
        "\n",
        "        self.key_layer = nn.Linear(key_size, hidden_size, bias=False)\n",
        "        self.query_layer = nn.Linear(query_size, hidden_size, bias=False)\n",
        "        self.energy_layer = nn.Linear(hidden_size, 1, bias=False)\n",
        "        \n",
        "        # to store attention scores\n",
        "        self.alphas = None\n",
        "        \n",
        "    def forward(self, query=None, proj_key=None, value=None, mask=None):\n",
        "        assert mask is not None, \"mask is required\"\n",
        "\n",
        "        # We first project the query (the decoder state).\n",
        "        # The projected keys (the encoder states) were already pre-computated.\n",
        "        query = self.query_layer(query)\n",
        "        \n",
        "        # Calculate scores.\n",
        "        scores = self.energy_layer(torch.tanh(query + proj_key))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "        \n",
        "        # Mask out invalid positions.\n",
        "        # The mask marks valid positions so we invert it using `mask & 0`.\n",
        "        scores.data.masked_fill_(mask == 0, -float('inf'))\n",
        "        \n",
        "        # Turn scores to probabilities.\n",
        "        alphas = F.softmax(scores, dim=-1)\n",
        "        self.alphas = alphas        \n",
        "        \n",
        "        # The context vector is the weighted sum of the values.\n",
        "        context = torch.bmm(alphas, value)\n",
        "        \n",
        "        # context shape: [B, 1, 2D], alphas shape: [B, 1, M]\n",
        "        return context, alphas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvaGqLoUl66O"
      },
      "source": [
        "## Full Model\n",
        "\n",
        "Here we define a function from hyperparameters to a full model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcz1G8zml66O"
      },
      "source": [
        "def make_model(src_vocab, tgt_vocab, emb_size=256, hidden_size=512, num_layers=1, dropout=0.1):\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "\n",
        "    attention = BahdanauAttention(hidden_size)\n",
        "\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(emb_size, hidden_size, num_layers=num_layers, dropout=dropout),\n",
        "        Decoder(emb_size, hidden_size, attention, num_layers=num_layers, dropout=dropout),\n",
        "        nn.Embedding(src_vocab, emb_size),\n",
        "        nn.Embedding(tgt_vocab, emb_size),\n",
        "        Generator(hidden_size, tgt_vocab))\n",
        "\n",
        "    return model.cuda() if USE_CUDA else model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHC24AXal66O"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1UoeEv1l66P"
      },
      "source": [
        "## Batches and Masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOL7PayJl66P"
      },
      "source": [
        "class Batch:\n",
        "    \"\"\"Object for holding a batch of data with mask during training.\n",
        "    Input is a batch from a torch text iterator.\n",
        "    \"\"\"\n",
        "    def __init__(self, src, trg, pad_index=0):\n",
        "        #print(\"len(src) = \", len(src))\n",
        "        #print(src)\n",
        "        src, src_lengths = src\n",
        "        \n",
        "        self.src = src\n",
        "        self.src_lengths = src_lengths\n",
        "        self.src_mask = (src != pad_index).unsqueeze(-2)\n",
        "        self.nseqs = src.size(0)\n",
        "        \n",
        "        self.trg = None\n",
        "        self.trg_y = None\n",
        "        self.trg_mask = None\n",
        "        self.trg_lengths = None\n",
        "        self.ntokens = None\n",
        "\n",
        "        if trg is not None:\n",
        "            trg, trg_lengths = trg\n",
        "            self.trg = trg[:, :-1]\n",
        "            self.trg_lengths = trg_lengths\n",
        "            self.trg_y = trg[:, 1:]\n",
        "            self.trg_mask = (self.trg_y != pad_index)\n",
        "            self.ntokens = (self.trg_y != pad_index).data.sum().item()\n",
        "        \n",
        "        if USE_CUDA:\n",
        "            self.src = self.src.cuda()\n",
        "            self.src_mask = self.src_mask.cuda()\n",
        "\n",
        "            if trg is not None:\n",
        "                self.trg = self.trg.cuda()\n",
        "                self.trg_y = self.trg_y.cuda()\n",
        "                self.trg_mask = self.trg_mask.cuda()\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXUwP3-bl66P"
      },
      "source": [
        "## Training Loop\n",
        "The code below trains the model for 1 epoch (=1 pass through the training data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS8h3tMbl66Q"
      },
      "source": [
        "def run_epoch(data_iter, model, loss_compute, print_every=50):\n",
        "    \"\"\"Standard Training and Logging Function\"\"\"\n",
        "\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    print_tokens = 0\n",
        "\n",
        "    for i, batch in enumerate(data_iter, 1):\n",
        "        \n",
        "        out, _, pre_output = model.forward(batch.src, batch.trg,\n",
        "                                           batch.src_mask, batch.trg_mask,\n",
        "                                           batch.src_lengths, batch.trg_lengths)\n",
        "        loss = loss_compute(pre_output, batch.trg_y, batch.nseqs)\n",
        "        total_loss += loss\n",
        "        total_tokens += batch.ntokens\n",
        "        print_tokens += batch.ntokens\n",
        "        \n",
        "        if model.training and i % print_every == 0:\n",
        "            elapsed = time.time() - start\n",
        "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
        "                    (i, loss / batch.nseqs, print_tokens / elapsed))\n",
        "            start = time.time()\n",
        "            print_tokens = 0\n",
        "\n",
        "    return math.exp(total_loss / float(total_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XibEOUd6l66Q"
      },
      "source": [
        "## Training Data and Batching\n",
        "\n",
        "We will use torch text for batching."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLGYmjfXl66Q"
      },
      "source": [
        "## Optimizer\n",
        "\n",
        "We will use the [Adam optimizer](https://arxiv.org/abs/1412.6980) with default settings ($\\beta_1=0.9$, $\\beta_2=0.999$ and $\\epsilon=10^{-8}$).\n",
        "\n",
        "We will use $0.0003$ as the learning rate here. You will have to tune that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFxMG4Mzl66R"
      },
      "source": [
        "## Loss Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NkO6Waml66R"
      },
      "source": [
        "class SimpleLossCompute:\n",
        "    \"\"\"A simple loss compute and train function.\"\"\"\n",
        "\n",
        "    def __init__(self, generator, criterion, opt=None):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "\n",
        "    def __call__(self, x, y, norm):\n",
        "        x = self.generator(x)\n",
        "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
        "                              y.contiguous().view(-1))\n",
        "        loss = loss / norm\n",
        "\n",
        "        if self.opt is not None:\n",
        "            loss.backward()          \n",
        "            self.opt.step()\n",
        "            self.opt.zero_grad()\n",
        "\n",
        "        return loss.data.item() * norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtpl3CaDl66R"
      },
      "source": [
        "### Printing examples\n",
        "\n",
        "To monitor progress during training, we will translate a few examples.\n",
        "\n",
        "We use greedy decoding for simplicity; that is, at each time step, starting at the first token, we choose the one with that maximum probability, and we never revisit that choice. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmYgHX3gl66R"
      },
      "source": [
        "def greedy_decode(model, src, src_mask, src_lengths, max_len=100, sos_index=1, eos_index=None):\n",
        "    \"\"\"Greedily decode a sentence.\"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_hidden, encoder_final = model.encode(src, src_mask, src_lengths)\n",
        "        prev_y = torch.ones(1, 1).fill_(sos_index).type_as(src)\n",
        "        trg_mask = torch.ones_like(prev_y)\n",
        "\n",
        "    output = []\n",
        "    attention_scores = []\n",
        "    hidden = None\n",
        "\n",
        "    for i in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            out, hidden, pre_output = model.decode(\n",
        "              encoder_hidden, encoder_final, src_mask,\n",
        "              prev_y, trg_mask, hidden)\n",
        "\n",
        "            # we predict from the pre-output layer, which is\n",
        "            # a combination of Decoder state, prev emb, and context\n",
        "            prob = model.generator(pre_output[:, -1])\n",
        "\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.data.item()\n",
        "        output.append(next_word)\n",
        "        prev_y = torch.ones(1, 1).type_as(src).fill_(next_word)\n",
        "        attention_scores.append(model.decoder.attention.alphas.cpu().numpy())\n",
        "    \n",
        "    output = np.array(output)\n",
        "        \n",
        "    # cut off everything starting from </s> \n",
        "    # (only when eos_index provided)\n",
        "    if eos_index is not None:\n",
        "        first_eos = np.where(output==eos_index)[0]\n",
        "        if len(first_eos) > 0:\n",
        "            output = output[:first_eos[0]]      \n",
        "    \n",
        "    return output, np.concatenate(attention_scores, axis=1)\n",
        "  \n",
        "\n",
        "def lookup_words(x, vocab=None):\n",
        "    if vocab is not None:\n",
        "        x = [vocab.itos[i] for i in x]\n",
        "\n",
        "    return [str(t) for t in x]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUAEmDwkmo0p"
      },
      "source": [
        "def print_examples(example_iter, model, n=2, max_len=100, \n",
        "                   sos_index=1, \n",
        "                   src_eos_index=None, \n",
        "                   trg_eos_index=None, \n",
        "                   src_vocab=None, trg_vocab=None):\n",
        "    \"\"\"Prints N examples. Assumes batch size of 1.\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    count = 0\n",
        "    print()\n",
        "    \n",
        "    if src_vocab is not None and trg_vocab is not None:\n",
        "        src_eos_index = src_vocab.stoi['<eos>']\n",
        "        trg_sos_index = trg_vocab.stoi['<bos>']\n",
        "        trg_eos_index = trg_vocab.stoi['<eos>']\n",
        "    else:\n",
        "        src_eos_index = None\n",
        "        trg_sos_index = 1\n",
        "        trg_eos_index = None\n",
        "        \n",
        "    for i, batch in enumerate(example_iter):\n",
        "      \n",
        "        src = batch.src.cpu().numpy()[0, :]\n",
        "        trg = batch.trg_y.cpu().numpy()[0, :]\n",
        "\n",
        "        # remove </s> (if it is there)\n",
        "        src = src[:-1] if src[-1] == src_eos_index else src\n",
        "        trg = trg[:-1] if trg[-1] == trg_eos_index else trg      \n",
        "      \n",
        "        result, _ = greedy_decode(\n",
        "          model, batch.src, batch.src_mask, batch.src_lengths,\n",
        "          max_len=max_len, sos_index=trg_sos_index, eos_index=trg_eos_index)\n",
        "        print(\"Example #%d\" % (i+1))\n",
        "        print(\"Src : \", \" \".join(lookup_words(src, vocab=src_vocab)))\n",
        "        print(\"Trg : \", \" \".join(lookup_words(trg, vocab=trg_vocab)))\n",
        "        print(\"Pred: \", \" \".join(lookup_words(result, vocab=trg_vocab)))\n",
        "        print()\n",
        "        \n",
        "        count += 1\n",
        "        if count == n:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89VnQsiOl66T"
      },
      "source": [
        "# Morphological inflection dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBFnEYbBl66U"
      },
      "source": [
        "#pip install git+git://github.com/pytorch/text spacy \n",
        "!pip uninstall torchtext\n",
        "!pip install torchtext==0.4.0\n",
        "!pip install spacy\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTxMqa8JSqUl",
        "outputId": "7a492de6-3e28-42e2-f07f-4304646c4251"
      },
      "source": [
        "import torchtext\n",
        "print(torchtext.__version__)\n",
        "from torchtext import data\n",
        "\n",
        "def read_data(example_file, datafields, lem_col=0, word_col=2, feat_col=1, feature_sep=','):\n",
        "  with open(example_file, encoding='utf-8') as f:\n",
        "    examples = []\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "\n",
        "        l_w_f = line.split('\\t')\n",
        "        lemma = l_w_f[lem_col]\n",
        "        word = l_w_f[word_col]\n",
        "        features = l_w_f[feat_col].split(feature_sep)\n",
        "            \n",
        "        src = list(lemma) + ['<features>'] + features\n",
        "        trg = list(word)\n",
        "            \n",
        "        examples.append(torchtext.data.Example.fromlist([src, trg], datafields))\n",
        "  return torchtext.data.Dataset(examples, datafields)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYta7LtPUuAH"
      },
      "source": [
        "SRC = torchtext.data.Field(init_token='<bos>', eos_token='<eos>',\n",
        "                               sequential=True, batch_first=True, include_lengths=True, pad_token='<pad>')\n",
        "TRG = torchtext.data.Field(init_token='<bos>', eos_token='<eos>',\n",
        "                                sequential=True, batch_first=True, include_lengths=True, unk_token=None, pad_token='<pad>')\n",
        "\n",
        "\n",
        "datafields = [('src', SRC), ('trg', TRG)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhYrMDQOVpin",
        "outputId": "370f6733-10b4-4888-ed25-ac2ed96eaf99"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/data', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PgKElQRRXvMW",
        "outputId": "d993ae17-f981-4071-9989-e0f4a4a68491"
      },
      "source": [
        "#load train dataset\n",
        "traindataset = read_data(\"/data/My Drive/data/russian-task1-train\", datafields)\n",
        "print(len(traindataset))\n",
        "\n",
        "print(\"First 5 training examples:\")\n",
        "for i in range(0, 5):\n",
        "  print(\"src:\", \" \".join(vars(traindataset[i])['src']))\n",
        "  print(\"trg:\", \" \".join(vars(traindataset[i])['trg']))\n",
        "\n",
        "#load dev dataset\n",
        "devdataset = read_data(\"/data/My Drive/data/russian-task1-dev\", datafields)\n",
        "print(len(devdataset))\n",
        "\n",
        "print(\"First 5 dev examples:\")\n",
        "for i in range(0, 5):\n",
        "  print(\"src:\", \" \".join(vars(devdataset[i])['src']))\n",
        "  print(\"trg:\", \" \".join(vars(devdataset[i])['trg']))\n",
        "\n",
        "#build vocabulary\n",
        "SRC.build_vocab(traindataset.src)\n",
        "TRG.build_vocab(traindataset.trg)\n",
        "\n",
        "print(\"Length of in_char vocabulary:\", len(SRC.vocab))\n",
        "print(\"Length of out_char vocabulary:\", len(TRG.vocab))\n",
        "\n",
        "#create iterator\n",
        "training_iter = data.BucketIterator(traindataset, batch_size=64, train=True, \n",
        "                                 sort_within_batch=True, \n",
        "                                 sort_key=lambda x: (len(x.src), len(x.trg)), repeat=False,\n",
        "                                 device=DEVICE)\n",
        "\n",
        "validation_iter = data.Iterator(devdataset, batch_size=1, train=False, sort=False, repeat=False, \n",
        "                           device=DEVICE)\n",
        "\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "PAD_INDEX = TRG.vocab.stoi[PAD_TOKEN]\n",
        "print(\"PAD_INDEX: \",PAD_INDEX)\n",
        "\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12390\n",
            "First 5 training examples:\n",
            "src: а а к <features> pos=N case=GEN num=SG\n",
            "trg: а а к а\n",
            "src: а а м <features> pos=N case=INS num=PL\n",
            "trg: а а м а м и\n",
            "src: а а р о н о в е ц <features> pos=N case=GEN num=PL\n",
            "trg: а а р о н о в ц е в\n",
            "src: а б а а с <features> pos=N case=INS num=PL\n",
            "trg: а б а а с а м и\n",
            "src: а б а д а н е ц <features> pos=N case=DAT num=PL\n",
            "trg: а б а д а н ц а м\n",
            "1591\n",
            "First 5 dev examples:\n",
            "src: а а р о н о в е ц <features> pos=N case=ESS num=SG\n",
            "trg: а а р о н о в ц е\n",
            "src: а б о н е м е н т н ы й <features> pos=ADJ case=GEN gen=MASC num=SG\n",
            "trg: а б о н е м е н т н о г о\n",
            "src: а б с е н т и з м <features> pos=N case=DAT num=PL\n",
            "trg: а б с е н т и з м а м\n",
            "src: а б с о л ю т и з м <features> pos=N case=ESS num=SG\n",
            "trg: а б с о л ю т и з м е\n",
            "src: а б с о л ю т и с т <features> pos=N case=GEN num=SG\n",
            "trg: а б с о л ю т и с т а\n",
            "Length of in_char vocabulary: 69\n",
            "Length of out_char vocabulary: 36\n",
            "PAD_INDEX:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuAh4S1leYKX"
      },
      "source": [
        "class Batch2:\n",
        "    \"\"\"Object for holding a batch of data with mask during training.\n",
        "    Input is a batch from a torch text iterator.\n",
        "    \"\"\"\n",
        "    def __init__(self, src, trg, pad_index=0):\n",
        "        #print(\"len(src) = \", len(src))\n",
        "        #print(src.shape)\n",
        "        #src_lengths = [1]*64\n",
        "        src, src_lengths = src\n",
        "\n",
        "        self.src = src\n",
        "        self.src_lengths = src_lengths\n",
        "        self.src_mask = (src != pad_index).unsqueeze(-2)\n",
        "        self.nseqs = src.size(0)\n",
        "        \n",
        "        self.trg = None\n",
        "        self.trg_y = None\n",
        "        self.trg_mask = None\n",
        "        self.trg_lengths = None\n",
        "        self.ntokens = None\n",
        "\n",
        "        if trg is not None:\n",
        "            trg, trg_lengths = trg\n",
        "            self.trg = trg[:, :-1]\n",
        "            self.trg_lengths = trg_lengths\n",
        "            self.trg_y = trg[:, 1:]\n",
        "            self.trg_mask = (self.trg_y != pad_index)\n",
        "            self.ntokens = (self.trg_y != pad_index).data.sum().item()\n",
        "        \n",
        "        if USE_CUDA:\n",
        "            self.src = self.src.cuda()\n",
        "            self.src_mask = self.src_mask.cuda()\n",
        "\n",
        "            if trg is not None:\n",
        "                self.trg = self.trg.cuda()\n",
        "                self.trg_y = self.trg_y.cuda()\n",
        "                self.trg_mask = self.trg_mask.cuda()\n",
        "\n",
        "def rebatch2(pad_idx, batch):\n",
        "    \"\"\"Wrap torchtext batch into our own Batch class for pre-processing\"\"\"\n",
        "    return Batch2(batch.src, batch.trg, pad_idx)                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB7iCOg6iKXh"
      },
      "source": [
        "def train2(model, num_epochs=10, lr=0.0003, print_every=100):\n",
        "        \n",
        "    if USE_CUDA:\n",
        "        model.cuda()\n",
        "        print(\"using CUDA\")\n",
        "\n",
        "    # optionally add label smoothing; see the Annotated Transformer\n",
        "    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    dev_perplexities = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      \n",
        "        print(\"Epoch\", epoch)\n",
        "        model.train()\n",
        "        train_perplexity = run_epoch((rebatch2(PAD_INDEX, b) for b in training_iter), \n",
        "                                     model,\n",
        "                                     SimpleLossCompute(model.generator, criterion, optim),\n",
        "                                     print_every=print_every)\n",
        "        \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            print_examples((rebatch2(PAD_INDEX, x) for x in validation_iter), \n",
        "                           model, n=3, src_vocab=SRC.vocab, trg_vocab=TRG.vocab)        \n",
        "\n",
        "            dev_perplexity = run_epoch((rebatch2(PAD_INDEX, b) for b in validation_iter), \n",
        "                                       model, \n",
        "                                       SimpleLossCompute(model.generator, criterion, None))\n",
        "            print(\"Validation perplexity: %f\" % dev_perplexity)\n",
        "            dev_perplexities.append(dev_perplexity)\n",
        "        \n",
        "    return dev_perplexities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4QMaVkIvDnv",
        "outputId": "a45aa84f-3ba0-4823-a832-35aff99c57a2"
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_qK4q8jkDhi"
      },
      "source": [
        "!pip install torch==1.6.0 torchvision==0.7.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MoqXfp0v1rD"
      },
      "source": [
        "## Train for 20 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SF53A1ndiR91",
        "outputId": "26693f29-e0b3-4714-cf7e-db27291e0930"
      },
      "source": [
        "#train\n",
        "model = make_model(len(SRC.vocab), len(TRG.vocab),\n",
        "                   emb_size=256, hidden_size=256,\n",
        "                   num_layers=1, dropout=0.2)\n",
        "dev_perplexities = train2(model, num_epochs=20, lr=0.0003)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using CUDA\n",
            "Epoch 0\n",
            "Epoch Step: 100 Loss: 12.649820 Tokens per Sec: 16472.511022\n",
            "\n",
            "Example #1\n",
            "Src :  <bos> а а р о н о в е ц <features> pos=N case=ESS num=SG\n",
            "Trg :  а а р о н о в ц е\n",
            "Pred:  а а р о н о в е\n",
            "\n",
            "Example #2\n",
            "Src :  <bos> а б о н е м е н т н ы й <features> pos=ADJ case=GEN gen=MASC num=SG\n",
            "Trg :  а б о н е м е н т н о г о\n",
            "Pred:  а б о н е м е н т н о й\n",
            "\n",
            "Example #3\n",
            "Src :  <bos> а б с е н т и з м <features> pos=N case=DAT num=PL\n",
            "Trg :  а б с е н т и з м а м\n",
            "Pred:  а б с е н т и з м у м\n",
            "\n",
            "Validation perplexity: 1.820628\n",
            "Epoch 1\n",
            "Epoch Step: 100 Loss: 3.437076 Tokens per Sec: 17471.879454\n",
            "\n",
            "Example #1\n",
            "Src :  <bos> а а р о н о в е ц <features> pos=N case=ESS num=SG\n",
            "Trg :  а а р о н о в ц е\n",
            "Pred:  а а р о н о в е ц е\n",
            "\n",
            "Example #2\n",
            "Src :  <bos> а б о н е м е н т н ы й <features> pos=ADJ case=GEN gen=MASC num=SG\n",
            "Trg :  а б о н е м е н т н о г о\n",
            "Pred:  а б о н е м е н т н о г о\n",
            "\n",
            "Example #3\n",
            "Src :  <bos> а б с е н т и з м <features> pos=N case=DAT num=PL\n",
            "Trg :  а б с е н т и з м а м\n",
            "Pred:  а б с е н т и з м м а м\n",
            "\n",
            "Validation perplexity: 1.294317\n",
            "Epoch 2\n",
            "Epoch Step: 100 Loss: 4.349242 Tokens per Sec: 18381.030808\n",
            "\n",
            "Example #1\n",
            "Src :  <bos> а а р о н о в е ц <features> pos=N case=ESS num=SG\n",
            "Trg :  а а р о н о в ц е\n",
            "Pred:  а а р о н о в е ц е\n",
            "\n",
            "Example #2\n",
            "Src :  <bos> а б о н е м е н т н ы й <features> pos=ADJ case=GEN gen=MASC num=SG\n",
            "Trg :  а б о н е м е н т н о г о\n",
            "Pred:  а б о н е м е н т н о г о\n",
            "\n",
            "Example #3\n",
            "Src :  <bos> а б с е н т и з м <features> pos=N case=DAT num=PL\n",
            "Trg :  а б с е н т и з м а м\n",
            "Pred:  а б с е н т и з м а м\n",
            "\n",
            "Validation perplexity: 1.180009\n",
            "Epoch 3\n",
            "Epoch Step: 100 Loss: 0.837142 Tokens per Sec: 16190.579613\n",
            "\n",
            "Example #1\n",
            "Src :  <bos> а а р о н о в е ц <features> pos=N case=ESS num=SG\n",
            "Trg :  а а р о н о в ц е\n",
            "Pred:  а а р о н о в е ц е\n",
            "\n",
            "Example #2\n",
            "Src :  <bos> а б о н е м е н т н ы й <features> pos=ADJ case=GEN gen=MASC num=SG\n",
            "Trg :  а б о н е м е н т н о г о\n",
            "Pred:  а б о н е м е н т н о г о\n",
            "\n",
            "Example #3\n",
            "Src :  <bos> а б с е н т и з м <features> pos=N case=DAT num=PL\n",
            "Trg :  а б с е н т и з м а м\n",
            "Pred:  а б с е н т и з м а м\n",
            "\n",
            "Validation perplexity: 1.121496\n",
            "Epoch 4\n",
            "Epoch Step: 100 Loss: 1.165379 Tokens per Sec: 18629.745809\n",
            "\n",
            "Example #1\n",
            "Src :  <bos> а а р о н о в е ц <features> pos=N case=ESS num=SG\n",
            "Trg :  а а р о н о в ц е\n",
            "Pred:  а а р о н о в ц е\n",
            "\n",
            "Example #2\n",
            "Src :  <bos> а б о н е м е н т н ы й <features> pos=ADJ case=GEN gen=MASC num=SG\n",
            "Trg :  а б о н е м е н т н о г о\n",
            "Pred:  а б о н е м е н т н о г о\n",
            "\n",
            "Example #3\n",
            "Src :  <bos> а б с е н т и з м <features> pos=N case=DAT num=PL\n",
            "Trg :  а б с е н т и з м а м\n",
            "Pred:  а б с е н т и з м а м\n",
            "\n",
            "Validation perplexity: 1.096978\n",
            "Epoch 5\n",
            "Epoch Step: 100 Loss: 0.439895 Tokens per Sec: 16776.366168\n",
            "\n",
            "Example #1\n",
            "Src :  <bos> а а р о н о в е ц <features> pos=N case=ESS num=SG\n",
            "Trg :  а а р о н о в ц е\n",
            "Pred:  а а р о н о в ц е\n",
            "\n",
            "Example #2\n",
            "Src :  <bos> а б о н е м е н т н ы й <features> pos=ADJ case=GEN gen=MASC num=SG\n",
            "Trg :  а б о н е м е н т н о г о\n",
            "Pred:  а б о н е м е н т н о г о\n",
            "\n",
            "Example #3\n",
            "Src :  <bos> а б с е н т и з м <features> pos=N case=DAT num=PL\n",
            "Trg :  а б с е н т и з м а м\n",
            "Pred:  а б с е н т и з м а м\n",
            "\n",
            "Validation perplexity: 1.080282\n",
            "Epoch 6\n",
            "Epoch Step: 100 Loss: 0.497333 Tokens per Sec: 18191.065690\n",
            "\n",
            "Example #1\n",
            "Src :  <bos> а а р о н о в е ц <features> pos=N case=ESS num=SG\n",
            "Trg :  а а р о н о в ц е\n",
            "Pred:  а а р о н о в ц е\n",
            "\n",
            "Example #2\n",
            "Src :  <bos> а б о н е м е н т н ы й <features> pos=ADJ case=GEN gen=MASC num=SG\n",
            "Trg :  а б о н е м е н т н о г о\n",
            "Pred:  а б о н е м е н т н о г о\n",
            "\n",
            "Example #3\n",
            "Src :  <bos> а б с е н т и з м <features> pos=N case=DAT num=PL\n",
            "Trg :  а б с е н т и з м а м\n",
            "Pred:  а б с е н т и з м а м\n",
            "\n",
            "Validation perplexity: 1.073828\n",
            "Epoch 7\n",
            "Epoch Step: 100 Loss: 0.427060 Tokens per Sec: 16686.857327\n",
            "\n",
            "Example #1\n",
            "Src :  <bos> а а р о н о в е ц <features> pos=N case=ESS num=SG\n",
            "Trg :  а а р о н о в ц е\n",
            "Pred:  а а р о н о в ц е\n",
            "\n",
            "Example #2\n",
            "Src :  <bos> а б о н е м е н т н ы й <features> pos=ADJ case=GEN gen=MASC num=SG\n",
            "Trg :  а б о н е м е н т н о г о\n",
            "Pred:  а б о н е м е н т н о г о\n",
            "\n",
            "Example #3\n",
            "Src :  <bos> а б с е н т и з м <features> pos=N case=DAT num=PL\n",
            "Trg :  а б с е н т и з м а м\n",
            "Pred:  а б с е н т и з м а м\n",
            "\n",
            "Validation perplexity: 1.071341\n",
            "Epoch 8\n",
            "Epoch Step: 100 Loss: 0.217625 Tokens per Sec: 18474.929130\n",
            "\n",
            "Example #1\n",
            "Src :  <bos> а а р о н о в е ц <features> pos=N case=ESS num=SG\n",
            "Trg :  а а р о н о в ц е\n",
            "Pred:  а а р о н о в ц е\n",
            "\n",
            "Example #2\n",
            "Src :  <bos> а б о н е м е н т н ы й <features> pos=ADJ case=GEN gen=MASC num=SG\n",
            "Trg :  а б о н е м е н т н о г о\n",
            "Pred:  а б о н е м е н т н о г о\n",
            "\n",
            "Example #3\n",
            "Src :  <bos> а б с е н т и з м <features> pos=N case=DAT num=PL\n",
            "Trg :  а б с е н т и з м а м\n",
            "Pred:  а б с е н т и з м а м\n",
            "\n",
            "Validation perplexity: 1.068572\n",
            "Epoch 9\n",
            "Epoch Step: 100 Loss: 0.203924 Tokens per Sec: 16532.989532\n",
            "\n",
            "Example #1\n",
            "Src :  <bos> а а р о н о в е ц <features> pos=N case=ESS num=SG\n",
            "Trg :  а а р о н о в ц е\n",
            "Pred:  а а р о н о в ц е\n",
            "\n",
            "Example #2\n",
            "Src :  <bos> а б о н е м е н т н ы й <features> pos=ADJ case=GEN gen=MASC num=SG\n",
            "Trg :  а б о н е м е н т н о г о\n",
            "Pred:  а б о н е м е н т н о г о\n",
            "\n",
            "Example #3\n",
            "Src :  <bos> а б с е н т и з м <features> pos=N case=DAT num=PL\n",
            "Trg :  а б с е н т и з м а м\n",
            "Pred:  а б с е н т и з м а м\n",
            "\n",
            "Validation perplexity: 1.062456\n",
            "Epoch 10\n",
            "Epoch Step: 100 Loss: 0.893449 Tokens per Sec: 18121.689000\n",
            "\n",
            "Example #1\n",
            "Src :  <bos> а а р о н о в е ц <features> pos=N case=ESS num=SG\n",
            "Trg :  а а р о н о в ц е\n",
            "Pred:  а а р о н о в ц е\n",
            "\n",
            "Example #2\n",
            "Src :  <bos> а б о н е м е н т н ы й <features> pos=ADJ case=GEN gen=MASC num=SG\n",
            "Trg :  а б о н е м е н т н о г о\n",
            "Pred:  а б о н е м е н т н о г о\n",
            "\n",
            "Example #3\n",
            "Src :  <bos> а б с е н т и з м <features> pos=N case=DAT num=PL\n",
            "Trg :  а б с е н т и з м а м\n",
            "Pred:  а б с е н т и з м а м\n",
            "\n",
            "Validation perplexity: 1.057026\n",
            "Epoch 11\n",
            "Epoch Step: 100 Loss: 0.680469 Tokens per Sec: 17539.630214\n",
            "\n",
            "Example #1\n",
            "Src :  <bos> а а р о н о в е ц <features> pos=N case=ESS num=SG\n",
            "Trg :  а а р о н о в ц е\n",
            "Pred:  а а р о н о в ц е\n",
            "\n",
            "Example #2\n",
            "Src :  <bos> а б о н е м е н т н ы й <features> pos=ADJ case=GEN gen=MASC num=SG\n",
            "Trg :  а б о н е м е н т н о г о\n",
            "Pred:  а б о н е м е н т н о г о\n",
            "\n",
            "Example #3\n",
            "Src :  <bos> а б с е н т и з м <features> pos=N case=DAT num=PL\n",
            "Trg :  а б с е н т и з м а м\n",
            "Pred:  а б с е н т и з м а м\n",
            "\n",
            "Validation perplexity: 1.078463\n",
            "Epoch 12\n",
            "Epoch Step: 100 Loss: 0.412399 Tokens per Sec: 18688.948010\n",
            "\n",
            "Example #1\n",
            "Src :  <bos> а а р о н о в е ц <features> pos=N case=ESS num=SG\n",
            "Trg :  а а р о н о в ц е\n",
            "Pred:  а а р о н о в ц е\n",
            "\n",
            "Example #2\n",
            "Src :  <bos> а б о н е м е н т н ы й <features> pos=ADJ case=GEN gen=MASC num=SG\n",
            "Trg :  а б о н е м е н т н о г о\n",
            "Pred:  а б о н е м е н т н о г о\n",
            "\n",
            "Example #3\n",
            "Src :  <bos> а б с е н т и з м <features> pos=N case=DAT num=PL\n",
            "Trg :  а б с е н т и з м а м\n",
            "Pred:  а б с е н т и з м а м\n",
            "\n",
            "Validation perplexity: 1.080607\n",
            "Epoch 13\n",
            "Epoch Step: 100 Loss: 0.552674 Tokens per Sec: 16455.826701\n",
            "\n",
            "Example #1\n",
            "Src :  <bos> а а р о н о в е ц <features> pos=N case=ESS num=SG\n",
            "Trg :  а а р о н о в ц е\n",
            "Pred:  а а р о н о в ц е\n",
            "\n",
            "Example #2\n",
            "Src :  <bos> а б о н е м е н т н ы й <features> pos=ADJ case=GEN gen=MASC num=SG\n",
            "Trg :  а б о н е м е н т н о г о\n",
            "Pred:  а б о н е м е н т н о г о\n",
            "\n",
            "Example #3\n",
            "Src :  <bos> а б с е н т и з м <features> pos=N case=DAT num=PL\n",
            "Trg :  а б с е н т и з м а м\n",
            "Pred:  а б с е н т и з м а м\n",
            "\n",
            "Validation perplexity: 1.054971\n",
            "Epoch 14\n",
            "Epoch Step: 100 Loss: 0.398595 Tokens per Sec: 18428.143724\n",
            "\n",
            "Example #1\n",
            "Src :  <bos> а а р о н о в е ц <features> pos=N case=ESS num=SG\n",
            "Trg :  а а р о н о в ц е\n",
            "Pred:  а а р о н о в ц е\n",
            "\n",
            "Example #2\n",
            "Src :  <bos> а б о н е м е н т н ы й <features> pos=ADJ case=GEN gen=MASC num=SG\n",
            "Trg :  а б о н е м е н т н о г о\n",
            "Pred:  а б о н е м е н т н о г о\n",
            "\n",
            "Example #3\n",
            "Src :  <bos> а б с е н т и з м <features> pos=N case=DAT num=PL\n",
            "Trg :  а б с е н т и з м а м\n",
            "Pred:  а б с е н т и з м а м\n",
            "\n",
            "Validation perplexity: 1.051614\n",
            "Epoch 15\n",
            "Epoch Step: 100 Loss: 0.366789 Tokens per Sec: 16942.858285\n",
            "\n",
            "Example #1\n",
            "Src :  <bos> а а р о н о в е ц <features> pos=N case=ESS num=SG\n",
            "Trg :  а а р о н о в ц е\n",
            "Pred:  а а р о н о в ц е\n",
            "\n",
            "Example #2\n",
            "Src :  <bos> а б о н е м е н т н ы й <features> pos=ADJ case=GEN gen=MASC num=SG\n",
            "Trg :  а б о н е м е н т н о г о\n",
            "Pred:  а б о н е м е н т н о г о\n",
            "\n",
            "Example #3\n",
            "Src :  <bos> а б с е н т и з м <features> pos=N case=DAT num=PL\n",
            "Trg :  а б с е н т и з м а м\n",
            "Pred:  а б с е н т и з м а м\n",
            "\n",
            "Validation perplexity: 1.053794\n",
            "Epoch 16\n",
            "Epoch Step: 100 Loss: 0.394177 Tokens per Sec: 18500.481553\n",
            "\n",
            "Example #1\n",
            "Src :  <bos> а а р о н о в е ц <features> pos=N case=ESS num=SG\n",
            "Trg :  а а р о н о в ц е\n",
            "Pred:  а а р о н о в ц е\n",
            "\n",
            "Example #2\n",
            "Src :  <bos> а б о н е м е н т н ы й <features> pos=ADJ case=GEN gen=MASC num=SG\n",
            "Trg :  а б о н е м е н т н о г о\n",
            "Pred:  а б о н е м е н т н о г о\n",
            "\n",
            "Example #3\n",
            "Src :  <bos> а б с е н т и з м <features> pos=N case=DAT num=PL\n",
            "Trg :  а б с е н т и з м а м\n",
            "Pred:  а б с е н т и з м а м\n",
            "\n",
            "Validation perplexity: 1.051583\n",
            "Epoch 17\n",
            "Epoch Step: 100 Loss: 0.350869 Tokens per Sec: 16648.174308\n",
            "\n",
            "Example #1\n",
            "Src :  <bos> а а р о н о в е ц <features> pos=N case=ESS num=SG\n",
            "Trg :  а а р о н о в ц е\n",
            "Pred:  а а р о н о в ц е\n",
            "\n",
            "Example #2\n",
            "Src :  <bos> а б о н е м е н т н ы й <features> pos=ADJ case=GEN gen=MASC num=SG\n",
            "Trg :  а б о н е м е н т н о г о\n",
            "Pred:  а б о н е м е н т н о г о\n",
            "\n",
            "Example #3\n",
            "Src :  <bos> а б с е н т и з м <features> pos=N case=DAT num=PL\n",
            "Trg :  а б с е н т и з м а м\n",
            "Pred:  а б с е н т и з м а м\n",
            "\n",
            "Validation perplexity: 1.053120\n",
            "Epoch 18\n",
            "Epoch Step: 100 Loss: 0.121427 Tokens per Sec: 18128.484011\n",
            "\n",
            "Example #1\n",
            "Src :  <bos> а а р о н о в е ц <features> pos=N case=ESS num=SG\n",
            "Trg :  а а р о н о в ц е\n",
            "Pred:  а а р о н о в ц е\n",
            "\n",
            "Example #2\n",
            "Src :  <bos> а б о н е м е н т н ы й <features> pos=ADJ case=GEN gen=MASC num=SG\n",
            "Trg :  а б о н е м е н т н о г о\n",
            "Pred:  а б о н е м е н т н о г о\n",
            "\n",
            "Example #3\n",
            "Src :  <bos> а б с е н т и з м <features> pos=N case=DAT num=PL\n",
            "Trg :  а б с е н т и з м а м\n",
            "Pred:  а б с е н т и з м а м\n",
            "\n",
            "Validation perplexity: 1.058001\n",
            "Epoch 19\n",
            "Epoch Step: 100 Loss: 0.235232 Tokens per Sec: 17107.744563\n",
            "\n",
            "Example #1\n",
            "Src :  <bos> а а р о н о в е ц <features> pos=N case=ESS num=SG\n",
            "Trg :  а а р о н о в ц е\n",
            "Pred:  а а р о н о в ц е\n",
            "\n",
            "Example #2\n",
            "Src :  <bos> а б о н е м е н т н ы й <features> pos=ADJ case=GEN gen=MASC num=SG\n",
            "Trg :  а б о н е м е н т н о г о\n",
            "Pred:  а б о н е м е н т н о г о\n",
            "\n",
            "Example #3\n",
            "Src :  <bos> а б с е н т и з м <features> pos=N case=DAT num=PL\n",
            "Trg :  а б с е н т и з м а м\n",
            "Pred:  а б с е н т и з м а м\n",
            "\n",
            "Validation perplexity: 1.055112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbpNy8K2l66W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "fdcbed44-b221-49d0-ce3c-5cb1b83ec255"
      },
      "source": [
        "def plot_perplexity(perplexities):\n",
        "    \"\"\"plot perplexities\"\"\"\n",
        "    plt.title(\"Perplexity per Epoch\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Perplexity\")\n",
        "    plt.plot(perplexities)\n",
        "\n",
        "plot_perplexity(dev_perplexities)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zyoTw-0l66W"
      },
      "source": [
        "## Prediction and Evaluation\n",
        "\n",
        "Once trained we can use the model to produce a set of translations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "k-6lhKLs598v",
        "outputId": "e2dbc7ce-f8f6-406e-e555-8ced889ff46a"
      },
      "source": [
        "golden = [\"\".join(example.trg) for example in devdataset]\n",
        "print(len(golden))\n",
        "print(golden[0])"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1591\n",
            "аароновце\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2nzmkJkl66Y"
      },
      "source": [
        "**Now we translate the validation set!**\n",
        "\n",
        "This might take a little bit of time.\n",
        "\n",
        "Note that `greedy_decode` will cut-off the sentence when it encounters the end-of-sequence symbol, if we provide it the index of that symbol."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtBRn-x0l66Y"
      },
      "source": [
        "hypotheses = []\n",
        "alphas = []  # save the last attention scores\n",
        "for batch in validation_iter:\n",
        "  batch = rebatch2(PAD_INDEX, batch)\n",
        "  pred, attention = greedy_decode(\n",
        "    model, batch.src, batch.src_mask, batch.src_lengths, max_len=25,\n",
        "    sos_index=TRG.vocab.stoi['<bos>'],\n",
        "    eos_index=TRG.vocab.stoi['<eos>'])\n",
        "  hypotheses.append(pred)\n",
        "  alphas.append(attention)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCPUQElIl66Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "72cc1d08-8010-44a1-ace2-7605e578a430"
      },
      "source": [
        "# we will still need to convert the indices to actual words!\n",
        "hypotheses[0]"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 4,  4,  9,  3,  7,  3, 13, 31,  6])"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5L4vHITl66Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f2027587-fb6a-49b7-df66-cd5572ef804c"
      },
      "source": [
        "hypotheses = [lookup_words(x, TRG.vocab) for x in hypotheses]\n",
        "hypotheses[0]"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['а', 'а', 'р', 'о', 'н', 'о', 'в', 'ц', 'е']"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MnbMMcw65oK9",
        "outputId": "79d5fc6e-746b-44f3-b430-c96cd6d182f9"
      },
      "source": [
        "guesses = [\"\".join(x) for x in hypotheses]\n",
        "print(len(guesses))\n",
        "print(guesses[0])\n"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1591\n",
            "аароновце\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "q8Rob6vU00aD",
        "outputId": "85cfea9e-a1f3-45ab-91e9-73dd4745aeb6"
      },
      "source": [
        "print(len(golden))\n",
        "print(len(guesses))"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1591\n",
            "1591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrQTm15P_8eu"
      },
      "source": [
        "!pip install python-Levenshtein\n",
        "from Levenshtein import distance as lev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "R3bFegfTr2iz",
        "outputId": "381103d5-558d-4d50-f4b9-ef901e588385"
      },
      "source": [
        "# the Levenshtein distance \n",
        "levsum=0\n",
        "levsumnorm=0\n",
        "accsum=0\n",
        "for i in range (0, len(golden)):\n",
        "  #evaluate a single tag\n",
        "  str1 = golden[i]\n",
        "  str2 = guesses[i]\n",
        "  levdist = lev(str1, str2)\n",
        "  acc = 1.0 if levdist==0 else 0\n",
        "  #print(\"levdist[\",i,\"] = \", levdist)\n",
        "  levsum+=levdist\n",
        "  levsumnorm+=levdist/len(str1)\n",
        "  accsum+=acc\n",
        "  #print(\"Sum levenstein distance = \", levsum)\n",
        "levaverage = levsum/len(golden)\n",
        "levaveragenorm = levsumnorm/len(golden)\n",
        "accuracy = accsum/len(golden)\n",
        "print(\"Accuracy = \", accuracy)\n",
        "print(\"Mean Levenshtein distance = \", levaverage)\n",
        "print(\"Mean normalized Levenshtein distance = \", levaveragenorm)\n"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy =  0.9176618478944061\n",
            "Mean Levenshtein distance =  0.13890634820867379\n",
            "Mean normalized Levenshtein distance =  0.01840912099120032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV9MVThsJ_Vu"
      },
      "source": [
        "The baseline accuracy was determined using the official sigmorphon2016 code (https://github.com/ryancotterell/sigmorphon2016/tree/master/src/baseline) which was downloaded and compiled and run on Ubuntu linux:\n",
        "\n",
        "Aggregate\n",
        "\n",
        "Accuracy: 0.897548711502\n",
        "\n",
        "Mean Levenshtein: 0.170333123821\n",
        "\n",
        "Mean Normalized Levenshtein: 0.0225829281062\n",
        "\n",
        "\n",
        "It is seen that the accuracy as well as the Levenshtein accuracy metrics obtained with the method implemented in this notebook are all better than the baseline. \n",
        "\n",
        "\n",
        "The accuracy obtained with the method implemented in this notebook  is slightly higher than the task 1 accuracy for the Russian language reported by Kann and Schutze (2016) (https://aclanthology.org/W16-2010.pdf) -- 91.77% vs. 91.46%."
      ]
    }
  ]
}